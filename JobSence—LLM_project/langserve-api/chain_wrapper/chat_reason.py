from dotenv import load_dotenv, find_dotenv
from langchain_community.chat_models.tongyi import ChatTongyi
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, END, MessagesState, StateGraph
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain.tools import Tool
import time
import os
import re
import json
import random

# å°è¯•å¯¼å…¥SerpAPIï¼Œå¦‚æœå¤±è´¥åˆ™æä¾›å¤‡é€‰æ–¹æ¡ˆ
try:
    from langchain_community.utilities import SerpAPIWrapper
    SERPAPI_AVAILABLE = True
except ImportError:
    print("âš ï¸ SerpAPIä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å†…ç½®çŸ¥è¯†å›ç­”é—®é¢˜")
    SERPAPI_AVAILABLE = False

_ = load_dotenv(find_dotenv())

# é…ç½®APIå¯†é’¥
SERPAPI_API_KEY = os.getenv("SERPAPI_API_KEY")

model = ChatTongyi(
    streaming=True,
    model_name="qwen-turbo"
)

# è”ç½‘æœç´¢åŠŸèƒ½ - è·å–é¢è¯•é¢˜ç›®
def web_search(query: str) -> dict:
    """ä½¿ç”¨SerpAPIè¿›è¡Œç½‘ç»œæœç´¢ï¼Œè¿”å›è¯¦ç»†çš„æœç´¢ç»“æœå’Œæ¥æºä¿¡æ¯"""
    try:
        print(f"[æ—¥å¿—] ğŸ” ç½‘ç»œæœç´¢é¢è¯•é¢˜ï¼Œå…³é”®è¯ï¼š{query}")
        
        if not SERPAPI_AVAILABLE:
            return {
                "success": False,
                "content": "ç½‘ç»œæœç´¢åŠŸèƒ½æš‚ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥SerpAPIé…ç½®",
                "sources": [],
                "total_results": 0
            }
            
        if not SERPAPI_API_KEY:
            return {
                "success": False,
                "content": "SerpAPIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œç½‘ç»œæœç´¢",
                "sources": [],
                "total_results": 0
            }
            
        search = SerpAPIWrapper()
        # è·å–åŸå§‹æœç´¢ç»“æœ
        raw_result = search.results(query + " é¢è¯•é¢˜")
        
        # è§£ææœç´¢ç»“æœï¼Œæå–è¯¦ç»†ä¿¡æ¯
        sources = []
        content_parts = []
        
        if "organic_results" in raw_result:
            for i, result in enumerate(raw_result["organic_results"][:8]):  # é™åˆ¶å‰8ä¸ªç»“æœ
                source = {
                    "id": i + 1,
                    "title": result.get("title", "æœªçŸ¥æ ‡é¢˜"),
                    "url": result.get("link", ""),
                    "snippet": result.get("snippet", "æ— æ‘˜è¦"),
                    "displayed_link": result.get("displayed_link", ""),
                    "favicon": result.get("favicon", ""),
                    "position": result.get("position", i + 1)
                }
                sources.append(source)
                content_parts.append(f"[{i+1}] {result.get('title', '')}: {result.get('snippet', '')}")
        
        # å¦‚æœæœ‰çŸ¥è¯†å›¾è°±ç»“æœï¼Œä¹ŸåŒ…å«è¿›æ¥
        if "knowledge_graph" in raw_result:
            kg = raw_result["knowledge_graph"]
            if "description" in kg:
                content_parts.insert(0, f"çŸ¥è¯†å›¾è°±: {kg['description']}")
        
        content = "\n\n".join(content_parts)
        
        print(f"[æ—¥å¿—] âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(sources)} ä¸ªé¢è¯•é¢˜ç›¸å…³ç»“æœ")
        
        return {
            "success": True,
            "content": content,
            "sources": sources,
            "total_results": len(sources),
            "query": query
        }
        
    except Exception as e:
        error_msg = f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        return {
            "success": False,
            "content": error_msg,
            "sources": [],
            "total_results": 0
        }

# æœ¬åœ°é¢è¯•é¢˜åº“æŸ¥è¯¢åŠŸèƒ½
def query_local_interview_questions(query: str) -> dict:
    """æŸ¥è¯¢æœ¬åœ°é¢è¯•é¢˜åº“ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœ"""
    try:
        print(f"\n[æ—¥å¿—] ğŸ“š æœ¬åœ°é¢è¯•é¢˜åº“æŸ¥è¯¢ï¼Œå…³é”®è¯ï¼š{query}")
        
        # æ¨¡æ‹Ÿæœ¬åœ°é¢è¯•é¢˜åº“
        interview_questions = {
            "python": [
                "è¯·è§£é‡ŠPythonä¸­çš„GILæ˜¯ä»€ä¹ˆï¼Œå®ƒå¯¹å¤šçº¿ç¨‹ç¼–ç¨‹æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ",
                "Pythonä¸­çš„è£…é¥°å™¨æ˜¯ä»€ä¹ˆï¼Ÿè¯·ç»™å‡ºä¸€ä¸ªç®€å•çš„ä¾‹å­ã€‚",
                "è¯·æè¿°Pythonä¸­çš„åˆ—è¡¨æ¨å¯¼å¼å’Œç”Ÿæˆå™¨è¡¨è¾¾å¼çš„åŒºåˆ«ã€‚",
                "å¦‚ä½•åœ¨Pythonä¸­å¤„ç†å¼‚å¸¸ï¼Ÿè¯·è§£é‡Štry-except-finallyçš„å·¥ä½œæµç¨‹ã€‚",
                "Pythonä¸­çš„æ·±æ‹·è´å’Œæµ…æ‹·è´æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
            ],
            "java": [
                "Javaä¸­çš„æ¥å£å’ŒæŠ½è±¡ç±»æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                "è¯·è§£é‡ŠJavaä¸­çš„åƒåœ¾å›æ”¶æœºåˆ¶ã€‚",
                "ä»€ä¹ˆæ˜¯Javaä¸­çš„çº¿ç¨‹å®‰å…¨ï¼Ÿå¦‚ä½•å®ç°çº¿ç¨‹å®‰å…¨ï¼Ÿ",
                "Javaä¸­çš„HashMapå’ŒConcurrentHashMapæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                "è¯·è§£é‡ŠJavaä¸­çš„åå°„æœºåˆ¶åŠå…¶åº”ç”¨åœºæ™¯ã€‚"
            ],
            "å‰ç«¯": [
                "è¯·è§£é‡ŠJavaScriptä¸­çš„é—­åŒ…æ¦‚å¿µåŠå…¶åº”ç”¨åœºæ™¯ã€‚",
                "Reactä¸­çš„è™šæ‹ŸDOMæ˜¯ä»€ä¹ˆï¼Ÿå®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
                "è¯·æè¿°CSSç›’æ¨¡å‹åŠå…¶ç»„æˆéƒ¨åˆ†ã€‚",
                "Vueå’ŒReactçš„ä¸»è¦åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
                "ä»€ä¹ˆæ˜¯è·¨åŸŸé—®é¢˜ï¼Ÿå¦‚ä½•è§£å†³è·¨åŸŸé—®é¢˜ï¼Ÿ"
            ],
            "æ•°æ®åº“": [
                "è¯·è§£é‡ŠSQLä¸­çš„ç´¢å¼•åŠå…¶å·¥ä½œåŸç†ã€‚",
                "ä»€ä¹ˆæ˜¯æ•°æ®åº“äº‹åŠ¡ï¼Ÿè¯·è§£é‡ŠACIDå±æ€§ã€‚",
                "NoSQLå’Œå…³ç³»å‹æ•°æ®åº“æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                "å¦‚ä½•ä¼˜åŒ–ä¸€ä¸ªæ…¢æŸ¥è¯¢SQLè¯­å¥ï¼Ÿ",
                "è¯·è§£é‡Šæ•°æ®åº“èŒƒå¼åŠå…¶ä½œç”¨ã€‚"
            ],
            "ç®—æ³•": [
                "è¯·è§£é‡Šæ—¶é—´å¤æ‚åº¦å’Œç©ºé—´å¤æ‚åº¦çš„æ¦‚å¿µã€‚",
                "ä»€ä¹ˆæ˜¯åŠ¨æ€è§„åˆ’ï¼Ÿè¯·ç»™å‡ºä¸€ä¸ªåº”ç”¨ä¾‹å­ã€‚",
                "è¯·æè¿°å¿«é€Ÿæ’åºçš„å·¥ä½œåŸç†åŠå…¶æ—¶é—´å¤æ‚åº¦ã€‚",
                "å¦‚ä½•åˆ¤æ–­ä¸€ä¸ªé“¾è¡¨æ˜¯å¦æœ‰ç¯ï¼Ÿ",
                "è¯·è§£é‡ŠäºŒå‰æ ‘çš„å‰åºã€ä¸­åºå’Œååºéå†ã€‚"
            ],
            "ç³»ç»Ÿè®¾è®¡": [
                "å¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜å¹¶å‘çš„ç³»ç»Ÿï¼Ÿ",
                "å¾®æœåŠ¡æ¶æ„çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                "å¦‚ä½•ä¿è¯åˆ†å¸ƒå¼ç³»ç»Ÿçš„ä¸€è‡´æ€§ï¼Ÿ",
                "è¯·è§£é‡ŠCAPå®šç†åŠå…¶åœ¨ç³»ç»Ÿè®¾è®¡ä¸­çš„åº”ç”¨ã€‚",
                "å¦‚ä½•è®¾è®¡ä¸€ä¸ªå¯æ‰©å±•çš„ç¼“å­˜ç³»ç»Ÿï¼Ÿ"
            ],
            "è½¯æŠ€èƒ½": [
                "è¯·æè¿°ä¸€ä¸ªä½ æ›¾ç»è§£å†³çš„æŠ€æœ¯éš¾é¢˜åŠå…¶è§£å†³è¿‡ç¨‹ã€‚",
                "å¦‚ä½•ä¸å›¢é˜Ÿæˆå‘˜æœ‰æ•ˆæ²Ÿé€šå’Œåä½œï¼Ÿ",
                "ä½ å¦‚ä½•ä¿æŒå¯¹æ–°æŠ€æœ¯çš„å­¦ä¹ å’Œæ›´æ–°ï¼Ÿ",
                "è¯·æè¿°ä¸€ä¸ªä½ å‚ä¸çš„é¡¹ç›®ï¼Œä»¥åŠä½ åœ¨å…¶ä¸­çš„è§’è‰²å’Œè´¡çŒ®ã€‚",
                "ä½ å¦‚ä½•å¤„ç†å·¥ä½œä¸­çš„å‹åŠ›å’ŒæŒ‘æˆ˜ï¼Ÿ"
            ]
        }
        
        # æ ¹æ®æŸ¥è¯¢å…³é”®è¯åŒ¹é…ç›¸å…³é¢˜ç›®
        matched_categories = []
        for category, questions in interview_questions.items():
            if category in query.lower() or any(keyword in query.lower() for keyword in category.lower().split()):
                matched_categories.append(category)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šç±»åˆ«ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªç±»åˆ«
        if not matched_categories:
            matched_categories = random.sample(list(interview_questions.keys()), 2)
        
        # ä»åŒ¹é…çš„ç±»åˆ«ä¸­é€‰æ‹©é—®é¢˜
        selected_questions = []
        for category in matched_categories:
            selected_questions.extend([(category, q) for q in interview_questions[category]])
        
        # éšæœºæ‰“ä¹±é—®é¢˜é¡ºåº
        random.shuffle(selected_questions)
        
        # æ„å»ºç»“æœ
        if selected_questions:
            content_parts = []
            for i, (category, question) in enumerate(selected_questions[:10], 1):
                content_parts.append(f"[{category}] {question}")
            
            result_content = "\n\n".join(content_parts)
            
            return {
                "success": True,
                "content": f"ä»æœ¬åœ°é¢è¯•é¢˜åº“ä¸­æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³é—®é¢˜ï¼š\n\n{result_content}",
                "sources": [{
                    "type": "local",
                    "file_name": "interview_questions_database",
                    "categories": matched_categories,
                    "question_count": len(content_parts)
                }],
                "total_results": len(content_parts)
            }
        else:
            return {
                "success": False,
                "content": "åœ¨æœ¬åœ°é¢è¯•é¢˜åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³é—®é¢˜ã€‚",
                "sources": [],
                "total_results": 0
            }
            
    except Exception as e:
        error_msg = f"æœ¬åœ°é¢è¯•é¢˜åº“æŸ¥è¯¢å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        return {
            "success": False,
            "content": error_msg,
            "sources": [],
            "total_results": 0
        }

# å®šä¹‰å·¥å…·åˆ—è¡¨
tools = [
    Tool(
        name="web_search",
        func=web_search,
        description="ç”¨äºæœç´¢æœ€æ–°çš„ITé¢è¯•é¢˜ç›®å’ŒæŠ€æœ¯é—®é¢˜ã€‚å½“éœ€è¦è·å–ç‰¹å®šæŠ€æœ¯é¢†åŸŸçš„é¢è¯•é¢˜æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚è¾“å…¥ï¼šæŠ€æœ¯é¢†åŸŸå…³é”®è¯"
    ),
    Tool(
        name="query_local_interview_questions", 
        func=query_local_interview_questions,
        description="ç”¨äºæŸ¥è¯¢æœ¬åœ°é¢è¯•é¢˜åº“ä¸­çš„é—®é¢˜ã€‚å½“éœ€è¦è·å–å¸¸è§ITé¢è¯•é¢˜ç›®æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚è¾“å…¥ï¼šæŠ€æœ¯é¢†åŸŸå…³é”®è¯"
    )
]

# å·¥å…·è°ƒç”¨å‡½æ•°
def call_tools(query: str, tool_name: str) -> dict:
    """æ ¹æ®å·¥å…·åç§°è°ƒç”¨ç›¸åº”çš„å·¥å…·ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœ"""
    for tool in tools:
        if tool.name == tool_name:
            return tool.func(query)  # ç›´æ¥è¿”å›å­—å…¸ç»“æœ
    return {
        "success": False,
        "content": f"æœªæ‰¾åˆ°å·¥å…·: {tool_name}",
        "sources": [],
        "total_results": 0
    }

# é¢è¯•çŠ¶æ€è·Ÿè¸ª
class InterviewState(MessagesState):
    """è·Ÿè¸ªé¢è¯•çŠ¶æ€çš„ç±»"""
    question_count: int = 0  # å·²æé—®çš„é—®é¢˜æ•°é‡
    current_topic: str = ""  # å½“å‰é¢è¯•ä¸»é¢˜
    asked_questions: list = []  # å·²æé—®çš„é—®é¢˜åˆ—è¡¨
    candidate_responses: list = []  # å€™é€‰äººçš„å›ç­”åˆ—è¡¨
    interview_feedback: list = []  # é¢è¯•å®˜çš„åé¦ˆåˆ—è¡¨
    interview_complete: bool = False  # é¢è¯•æ˜¯å¦å®Œæˆ

# æ™ºèƒ½å·¥å…·é€‰æ‹©å‡½æ•°
def select_and_call_tool(state):
    """æ™ºèƒ½é€‰æ‹©å·¥å…·å¹¶è°ƒç”¨"""
    print(f"[æ—¥å¿—] ğŸ“Š å½“å‰çŠ¶æ€: {state}")
    
    # è·å–çŠ¶æ€ä¿¡æ¯
    question_count = state.get("question_count", 0)
    current_topic = state.get("current_topic", "")
    asked_questions = state.get("asked_questions", [])
    interview_complete = state.get("interview_complete", False)
    messages = state.get("messages", [])
    
    # æ£€æŸ¥æ˜¯å¦åˆšåˆšç”Ÿæˆäº†é—®é¢˜ï¼ˆé¿å…é‡å¤ç”Ÿæˆï¼‰
    if messages and "ã€é¢è¯•å®˜ã€‘" in messages[-1].content:
        print("[æ—¥å¿—] â¸ï¸ å·²ç”Ÿæˆé—®é¢˜ï¼Œç­‰å¾…ç”¨æˆ·å›ç­”")
        return state  # ä¸ç”Ÿæˆæ–°é—®é¢˜ï¼Œä¿æŒå½“å‰çŠ¶æ€
    
    # æ£€æŸ¥é¢è¯•æ˜¯å¦å·²å®Œæˆ
    if question_count >= 8 or interview_complete:
        print("[æ—¥å¿—] âœ… é¢è¯•å·²å®Œæˆ")
        completion_message = HumanMessage(
            content="ã€é¢è¯•å®˜ã€‘é¢è¯•å·²ç»“æŸï¼Œæ„Ÿè°¢æ‚¨çš„å‚ä¸ï¼æˆ‘ä»¬ä¼šå°½å¿«ç»™æ‚¨åé¦ˆã€‚",
            additional_kwargs={"interview_complete": True}
        )
        return {
            "messages": [completion_message],
            "question_count": question_count,
            "current_topic": current_topic,
            "asked_questions": asked_questions,
            "interview_complete": True
        }
    
  # ç”Ÿæˆæ–°é—®é¢˜çš„é€»è¾‘ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
def evaluate_response(state):
        """è¯„ä¼°å€™é€‰äººå›ç­”"""
        print(f"[æ—¥å¿—] ğŸ” è¯„ä¼°å€™é€‰äººå›ç­”")
        
        # è·å–çŠ¶æ€ä¿¡æ¯
        messages = state.get("messages", [])
        question_count = state.get("question_count", 0)
        current_topic = state.get("current_topic", "")
        asked_questions = state.get("asked_questions", [])
        
        # âœ… å…³é”®ä¿®æ”¹ï¼šåªè¯„ä¼°çœŸå®çš„ç”¨æˆ·å›ç­”
        if not messages:
            print("[æ—¥å¿—] âš ï¸ æ²¡æœ‰æ¶ˆæ¯éœ€è¦è¯„ä¼°")
            return state
        
        last_message = messages[-1]
        
        # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦æ˜¯ç”¨æˆ·å›ç­”ï¼ˆä¸æ˜¯é¢è¯•å®˜é—®é¢˜ï¼‰
        if "ã€é¢è¯•å®˜ã€‘" in last_message.content:
            print("[æ—¥å¿—] âš ï¸ æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯é¢è¯•å®˜é—®é¢˜ï¼Œæ— éœ€è¯„ä¼°")
            return state
        
        # è·å–ç”¨æˆ·çš„çœŸå®å›ç­”
        candidate_answer = last_message.content
        current_question = asked_questions[-1] if asked_questions else "æœªçŸ¥é—®é¢˜"
        
        # æ„å»ºè¯„ä¼°æç¤º
        evaluation_prompt = f"""
        é¢è¯•é—®é¢˜ï¼š{current_question}
        å€™é€‰äººå›ç­”ï¼š{candidate_answer}
        
        è¯·å¯¹å€™é€‰äººçš„å›ç­”è¿›è¡Œä¸“ä¸šè¯„ä¼°ï¼ŒåŒ…æ‹¬ï¼š
        1. æŠ€æœ¯å‡†ç¡®æ€§
        2. æ·±åº¦ç†è§£
        3. å®è·µç»éªŒ
        4. è§£å†³é—®é¢˜èƒ½åŠ›
        5. è¡¨è¾¾èƒ½åŠ›
        
        ç»™å‡ºç®€æ´çš„è¯„ä¼°åé¦ˆã€‚
        """
        
        # ç”Ÿæˆè¯„ä¼°
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸“ä¸šçš„ITæŠ€æœ¯é¢è¯•å®˜ï¼Œè¯·å¯¹å€™é€‰äººçš„å›ç­”è¿›è¡Œå®¢è§‚è¯„ä¼°ã€‚"),
            ("human", evaluation_prompt)
        ])
        
        response = model.invoke(prompt.format_messages())
        
        # è¿”å›è¯„ä¼°ç»“æœ
        return {
            "messages": [response],
            "question_count": question_count,
            "current_topic": current_topic,
            "asked_questions": asked_questions,
            "interview_complete": question_count >= 8
        }
        # åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ 


def analyze_resume(state):
    """åˆ†æç”¨æˆ·ç®€å†å¹¶ç”Ÿæˆç¬¬ä¸€ä¸ªé—®é¢˜"""
    print(f"[æ—¥å¿—] ğŸ“„ åˆ†æç”¨æˆ·ç®€å†")
    
    messages = state.get("messages", [])
    if not messages:
        return state
    
    # è·å–ç”¨æˆ·è¾“å…¥çš„ç®€å†å†…å®¹
    user_input = messages[-1].content
    
    # é‡æ–°åˆ›å»ºåˆ†ææç¤ºè¯ï¼Œç¡®ä¿æ²¡æœ‰éšè—å­—ç¬¦
    analysis_text = "è¯·åˆ†æä»¥ä¸‹ç®€å†å†…å®¹ï¼Œå¹¶æ ¹æ®ç®€å†ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªé¢è¯•é—®é¢˜ï¼š\n\n"
    analysis_text += f"ç®€å†å†…å®¹ï¼š\n{user_input}\n\n"
    analysis_text += "è¯·ï¼š\n"
    analysis_text += "1. åˆ†æå€™é€‰äººçš„æŠ€æœ¯èƒŒæ™¯å’Œç»éªŒ\n"
    analysis_text += "2. è¯†åˆ«å…³é”®æŠ€èƒ½å’Œé¡¹ç›®ç»éªŒ\n"
    analysis_text += "3. ç”Ÿæˆä¸€ä¸ªé’ˆå¯¹æ€§çš„å¼€åœºé¢è¯•é—®é¢˜\n"
    analysis_text += "4. é—®é¢˜åº”è¯¥åŸºäºç®€å†å†…å®¹ï¼Œå…·æœ‰é’ˆå¯¹æ€§\n\n"
    analysis_text += "ç›´æ¥è¾“å‡ºé¢è¯•é—®é¢˜ï¼Œæ ¼å¼ï¼šã€é¢è¯•å®˜ã€‘é—®é¢˜å†…å®¹"
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸“ä¸šçš„ITæŠ€æœ¯é¢è¯•å®˜ï¼Œæ“…é•¿æ ¹æ®ç®€å†å†…å®¹æå‡ºé’ˆå¯¹æ€§é—®é¢˜ã€‚"),
        ("human", analysis_text)
    ])
    
    response = model.invoke(prompt.format_messages())
    print(f"[æ—¥å¿—] åˆ†æç»“æœ: {response.content}")
    return {
        "messages": [response],
        "resume_content": user_input,
        "resume_analyzed": True,
        "question_count": 1,
        "current_topic": "ç®€å†ç›¸å…³",
        "asked_questions": [response.content],
        "candidate_responses": [],
        "interview_feedback": [],
        "interview_complete": False
    }
def generate_next_question(state):
    """æ ¹æ®ç®€å†ã€é¢˜åº“å’Œç”¨æˆ·å›ç­”ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜"""
    print(f"[æ—¥å¿—] ğŸ¤” ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜")
    
    # è·å–çŠ¶æ€ä¿¡æ¯
    messages = state.get("messages", [])
    resume_content = state.get("resume_content", "")
    question_count = state.get("question_count", 0)
    asked_questions = state.get("asked_questions", [])
    candidate_responses = state.get("candidate_responses", [])
    
    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆé¢è¯•
    if question_count > 8:
        completion_message = AIMessage(
            content="ã€é¢è¯•å®˜ã€‘é¢è¯•å·²ç»“æŸï¼Œæ„Ÿè°¢æ‚¨çš„å‚ä¸ï¼æˆ‘ä»¬ä¼šå°½å¿«ç»™æ‚¨åé¦ˆã€‚"
        )
        return {
            "messages": [completion_message],
            "resume_content": resume_content,
            "resume_analyzed": True,
            "question_count": question_count,
            "current_topic": state.get("current_topic", ""),
            "asked_questions": asked_questions,
            "candidate_responses": candidate_responses,
            "interview_feedback": state.get("interview_feedback", []),
            "interview_complete": True
        }
    
    # è·å–æœ€æ–°çš„ç”¨æˆ·å›ç­”
    latest_response = ""
    if messages:
        latest_response = messages[-1].content
        candidate_responses.append(latest_response)
    
    # ä»ç®€å†ä¸­æå–æŠ€æœ¯å…³é”®è¯
    def extract_tech_keywords(resume_text):
        """ä»ç®€å†ä¸­æå–æŠ€æœ¯å…³é”®è¯"""
        tech_keywords = []
        common_techs = [
            "python", "java", "javascript", "react", "vue", "spring", "django", 
            "mysql", "redis", "mongodb", "docker", "kubernetes", "aws", "git",
            "linux", "ç®—æ³•", "æ•°æ®ç»“æ„", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "å¾®æœåŠ¡", "åˆ†å¸ƒå¼"
        ]
        
        resume_lower = resume_text.lower()
        for tech in common_techs:
            if tech in resume_lower:
                tech_keywords.append(tech)
        return tech_keywords
    
    # è¯»å–é¢˜åº“æ–‡ä»¶
    def load_question_bank():
        """ä»æ–‡ä»¶ä¸­åŠ è½½é¢è¯•é¢˜åº“"""
        import os
        
        question_bank = {
            "æŠ€æœ¯é—®é¢˜": [],
            "è¡Œä¸ºé—®é¢˜": []
        }
        
        try:
            # è¯»å–æŠ€æœ¯é¢è¯•é—®é¢˜
            tech_file_path = os.path.join(os.path.dirname(__file__), "interview", "interview_questions.txt")
            if os.path.exists(tech_file_path):
                with open(tech_file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # è§£ææ–‡ä»¶å†…å®¹ï¼Œæå–é—®é¢˜
                    lines = content.split('\n')
                    for line in lines:
                        line = line.strip()
                        if line.startswith('- "') and line.endswith('"'):
                            # æå–å¼•å·å†…çš„é—®é¢˜
                            question = line[3:-1]  # å»æ‰ '- "' å’Œ '"'
                            question_bank["æŠ€æœ¯é—®é¢˜"].append(question)
            
            # è¯»å–è¡Œä¸ºé¢è¯•é—®é¢˜
            behavior_file_path = os.path.join(os.path.dirname(__file__), "interview", "behavioral_questions.txt")
            if os.path.exists(behavior_file_path):
                with open(behavior_file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # è§£ææ–‡ä»¶å†…å®¹ï¼Œæå–é—®é¢˜
                    lines = content.split('\n')
                    for line in lines:
                        line = line.strip()
                        if line.startswith('- "') and line.endswith('"'):
                            # æå–å¼•å·å†…çš„é—®é¢˜
                            question = line[3:-1]  # å»æ‰ '- "' å’Œ '"'
                            question_bank["è¡Œä¸ºé—®é¢˜"].append(question)
                        elif line.startswith('- "') and '" (' in line:
                            # å¤„ç†å¸¦æœ‰è¯´æ˜çš„é—®é¢˜ï¼Œå¦‚ '- "é—®é¢˜å†…å®¹" (è¯´æ˜)'
                            question = line[3:line.find('" (')]  # æå–é—®é¢˜éƒ¨åˆ†
                            question_bank["è¡Œä¸ºé—®é¢˜"].append(question)
            
            print(f"[æ—¥å¿—] ğŸ“š æˆåŠŸåŠ è½½é¢˜åº“ï¼šæŠ€æœ¯é—®é¢˜ {len(question_bank['æŠ€æœ¯é—®é¢˜'])} ä¸ªï¼Œè¡Œä¸ºé—®é¢˜ {len(question_bank['è¡Œä¸ºé—®é¢˜'])} ä¸ª")
            
        except Exception as e:
            print(f"[é”™è¯¯] ğŸ“š åŠ è½½é¢˜åº“å¤±è´¥ï¼š{str(e)}")
            # å¦‚æœæ–‡ä»¶è¯»å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é¢˜åº“
            question_bank = {
                "æŠ€æœ¯é—®é¢˜": [
                    "è¯·è§£é‡Šä¸€ä¸‹å¿«é€Ÿæ’åºçš„åŸç†ã€‚",
                    "å¦‚ä½•åœ¨äºŒå‰æœç´¢æ ‘ä¸­æ‰¾åˆ°ç¬¬kå¤§çš„å…ƒç´ ï¼Ÿ",
                    "LRUç¼“å­˜æ·˜æ±°ç®—æ³•å¦‚ä½•å®ç°ï¼Ÿ"
                ],
                "è¡Œä¸ºé—®é¢˜": [
                    "è¯·æè¿°ä¸€ä¸ªä½ é‡åˆ°çš„æœ€å›°éš¾çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œä»¥åŠä½ æ˜¯å¦‚ä½•è§£å†³çš„ï¼Ÿ",
                    "è¯·åˆ†äº«ä¸€æ¬¡ä½ å’Œå›¢é˜Ÿæˆå‘˜å‘ç”Ÿæ„è§åˆ†æ­§çš„ç»å†ï¼Œä½ ä»¬æ˜¯å¦‚ä½•è¾¾æˆå…±è¯†çš„ï¼Ÿ"
                ]
            }
        
        return question_bank
    
    # åˆ¤æ–­é—®é¢˜ç”Ÿæˆæ¨¡å¼
    def determine_question_mode(latest_response, question_count):
        """åˆ¤æ–­åº”è¯¥ä½¿ç”¨å“ªç§é—®é¢˜ç”Ÿæˆæ¨¡å¼"""
        # æ¨¡å¼1ï¼šåŸºäºé¢˜åº“ç”Ÿæˆæ–°é—®é¢˜ï¼ˆå‰3ä¸ªé—®é¢˜æˆ–å›ç­”è¾ƒç®€çŸ­æ—¶ï¼‰
        if question_count <= 3 or len(latest_response) < 100:
            return "question_bank_mode"
        # æ¨¡å¼2ï¼šåŸºäºå›ç­”è¿½é—®æŠ€æœ¯ç»†èŠ‚
        else:
            return "follow_up_mode"
    
    # æå–æŠ€æœ¯å…³é”®è¯å’ŒåŠ è½½é¢˜åº“
    tech_keywords = extract_tech_keywords(resume_content)
    question_bank = load_question_bank()
    question_mode = determine_question_mode(latest_response, question_count)
    
    if question_mode == "question_bank_mode":
        # æ¨¡å¼1ï¼šåŸºäºç®€å†ä¸é¢è¯•é¢˜åº“ç”Ÿæˆæ–°é—®é¢˜ï¼ˆé¢˜åº“æƒé‡æ›´é«˜ï¼‰
        print(f"[æ—¥å¿—] ğŸ“š ä½¿ç”¨é¢˜åº“æ¨¡å¼ç”Ÿæˆé—®é¢˜")
        
        # ä»é¢˜åº“ä¸­é€‰æ‹©ç›¸å…³é—®é¢˜
        all_questions = question_bank["æŠ€æœ¯é—®é¢˜"] + question_bank["è¡Œä¸ºé—®é¢˜"]
        
        # è¿‡æ»¤å·²é—®è¿‡çš„é—®é¢˜
        available_questions = []
        for q in all_questions:
            # æ£€æŸ¥é—®é¢˜æ˜¯å¦å·²ç»è¢«é—®è¿‡ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
            is_asked = False
            for asked_q in asked_questions:
                if q in asked_q or asked_q in q:
                    is_asked = True
                    break
            if not is_asked:
                available_questions.append(q)
        
        if available_questions:
            # ä»å¯ç”¨é—®é¢˜ä¸­é€‰æ‹©ä¸€ä¸ªä¸ç®€å†æœ€ç›¸å…³çš„
            import random
            
            # ä¼˜å…ˆé€‰æ‹©ä¸æŠ€æœ¯å…³é”®è¯ç›¸å…³çš„é—®é¢˜
            relevant_questions = []
            for question in available_questions:
                for keyword in tech_keywords:
                    if keyword.lower() in question.lower():
                        relevant_questions.append(question)
                        break
            
            # å¦‚æœæœ‰ç›¸å…³é—®é¢˜å°±ä»ä¸­é€‰æ‹©ï¼Œå¦åˆ™éšæœºé€‰æ‹©
            if relevant_questions:
                selected_question = random.choice(relevant_questions)
            else:
                selected_question = random.choice(available_questions)
            
            question_prompt = f"""
            åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œè¯·ä¼˜åŒ–å¹¶ä¸ªæ€§åŒ–è¿™ä¸ªé¢è¯•é—®é¢˜ï¼š
            
            é¢˜åº“é—®é¢˜ï¼š{selected_question}
            
            æŠ€æœ¯å…³é”®è¯ï¼š{', '.join(tech_keywords)}
            
            è¦æ±‚ï¼š
            1. ä¿æŒé¢˜åº“é—®é¢˜çš„æ ¸å¿ƒæŠ€æœ¯ç‚¹
            2. ç¡®ä¿é—®é¢˜å…·æœ‰é’ˆå¯¹æ€§å’Œé€‚å½“çš„éš¾åº¦
            3. é¿å…ä¸ä¹‹å‰é—®é¢˜é‡å¤
            
            è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„é¢è¯•é—®é¢˜ï¼Œæ ¼å¼ï¼šã€é¢è¯•å®˜ã€‘é—®é¢˜å†…å®¹
            """
        else:
            # å¦‚æœé¢˜åº“é—®é¢˜éƒ½ç”¨å®Œäº†ï¼Œç”Ÿæˆæ–°é—®é¢˜
            question_prompt = f"""
            åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªæ–°çš„é¢è¯•é—®é¢˜ï¼š
            
            ç®€å†å†…å®¹ï¼š
            {resume_content}
            
            å·²æé—®çš„é—®é¢˜ï¼š
            {chr(10).join([f"{i+1}. {q}" for i, q in enumerate(asked_questions)])}
            
            æŠ€æœ¯å…³é”®è¯ï¼š{', '.join(tech_keywords)}
            
            è¯·ç”Ÿæˆç¬¬{question_count + 1}ä¸ªé—®é¢˜ï¼Œè¦æ±‚ï¼š
            1. åŸºäºç®€å†ä¸­çš„æŠ€æœ¯æ ˆå’Œé¡¹ç›®ç»éªŒ
            2. å…·æœ‰ä¸€å®šçš„æŠ€æœ¯æ·±åº¦å’ŒæŒ‘æˆ˜æ€§
            3. é¿å…é‡å¤ä¹‹å‰çš„é—®é¢˜
            4. é—®é¢˜åº”è¯¥æœ‰é’ˆå¯¹æ€§
            
            è¯·ç›´æ¥è¾“å‡ºé¢è¯•é—®é¢˜ï¼Œæ ¼å¼ï¼šã€é¢è¯•å®˜ã€‘é—®é¢˜å†…å®¹
            """
    
    else:
        # æ¨¡å¼2ï¼šåŸºäºç”¨æˆ·å›ç­”è¿½é—®æŠ€æœ¯ç»†èŠ‚
        print(f"[æ—¥å¿—] ğŸ” ä½¿ç”¨è¿½é—®æ¨¡å¼ç”Ÿæˆé—®é¢˜")
        
        question_prompt = f"""
        åŸºäºå€™é€‰äººçš„æœ€æ–°å›ç­”ï¼Œè¯·ç”Ÿæˆä¸€ä¸ªæ·±å…¥çš„è¿½é—®é—®é¢˜ï¼š
        
        å€™é€‰äººçš„æœ€æ–°å›ç­”ï¼š
        {latest_response}
        
        ç®€å†å†…å®¹ï¼š
        {resume_content}
        
        å·²æé—®çš„é—®é¢˜ï¼š
        {chr(10).join([f"{i+1}. {q}" for i, q in enumerate(asked_questions)])}
        
        æ‰€æœ‰å€™é€‰äººå›ç­”ï¼š
        {chr(10).join([f"{i+1}. {r}" for i, r in enumerate(candidate_responses)])}
        
        è¯·ç”Ÿæˆä¸€ä¸ªè¿½é—®é—®é¢˜ï¼Œè¦æ±‚ï¼š
        1. é’ˆå¯¹å€™é€‰äººå›ç­”ä¸­çš„å…³é”®æŠ€æœ¯ç‚¹è¿›è¡Œæ·±å…¥è¿½é—®
        2. æŒ–æ˜æ›´å¤šæŠ€æœ¯ç»†èŠ‚å’Œå®é™…åº”ç”¨ç»éªŒ
        3. æµ‹è¯•å€™é€‰äººå¯¹ç›¸å…³æŠ€æœ¯çš„æ·±åº¦ç†è§£
        4. å¯ä»¥è¯¢é—®å…·ä½“çš„å®ç°æ–¹æ¡ˆã€é‡åˆ°çš„é—®é¢˜ã€è§£å†³æ€è·¯ç­‰
        5. ä¿æŒé—®é¢˜çš„ä¸“ä¸šæ€§å’ŒæŒ‘æˆ˜æ€§
        
        è¯·ç›´æ¥è¾“å‡ºè¿½é—®é—®é¢˜ï¼Œæ ¼å¼ï¼šã€é¢è¯•å®˜ã€‘é—®é¢˜å†…å®¹
        """
    
    # ç”Ÿæˆé—®é¢˜
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸“ä¸šçš„ITæŠ€æœ¯é¢è¯•å®˜ï¼Œæ“…é•¿æ ¹æ®ç®€å†ã€é¢˜åº“å’Œå€™é€‰äººå›ç­”ç”Ÿæˆé«˜è´¨é‡çš„é¢è¯•é—®é¢˜ã€‚"),
        ("human", question_prompt)
    ])
    
    response = model.invoke(prompt.format_messages())
    
    # æ›´æ–°çŠ¶æ€
    new_asked_questions = asked_questions + [response.content]
    
    return {
        "messages": [response],
        "resume_content": resume_content,
        "resume_analyzed": True,
        "question_count": question_count + 1,
        "current_topic": "æŠ€æœ¯æ·±å…¥" if question_mode == "follow_up_mode" else "é¢˜åº“é—®é¢˜",
        "asked_questions": new_asked_questions,
        "candidate_responses": candidate_responses,
        "interview_feedback": state.get("interview_feedback", []),
        "interview_complete": False
    }
def final_evaluation(state):
    """ç”Ÿæˆæœ€ç»ˆé¢è¯•è¯„ä¼°æŠ¥å‘Š"""
    print(f"[æ—¥å¿—] ğŸ“Š ç”Ÿæˆæœ€ç»ˆé¢è¯•è¯„ä¼°æŠ¥å‘Š")
    
    # è·å–çŠ¶æ€ä¿¡æ¯
    messages = state.get("messages", [])
    resume_content = state.get("resume_content", "")
    asked_questions = state.get("asked_questions", [])
    candidate_responses = state.get("candidate_responses", [])
    question_count = state.get("question_count", 0)
    
    # æ„å»ºå®Œæ•´çš„é¢è¯•è®°å½•
    interview_record = ""
    for i, (question, response) in enumerate(zip(asked_questions, candidate_responses), 1):
        interview_record += f"\né—®é¢˜{i}ï¼š{question}\n"
        interview_record += f"å›ç­”{i}ï¼š{response}\n"
        interview_record += "-" * 50 + "\n"
    
    # æ„å»ºè¯„ä¼°æç¤ºè¯
    evaluation_prompt = f"""
    è¯·å¯¹ä»¥ä¸‹é¢è¯•è¿›è¡Œå…¨é¢çš„ä¸“ä¸šè¯„ä¼°ï¼š
    
    ã€ç®€å†ä¿¡æ¯ã€‘
    {resume_content}
    
    ã€é¢è¯•è®°å½•ã€‘
    {interview_record}
    
    ã€è¯„ä¼°è¦æ±‚ã€‘
    è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œä¸“ä¸šè¯„ä¼°ï¼š
    
    1. **æŠ€æœ¯èƒ½åŠ›è¯„ä¼°** (1-10åˆ†)
       - æŠ€æœ¯çŸ¥è¯†çš„å¹¿åº¦å’Œæ·±åº¦
       - å¯¹æ ¸å¿ƒæ¦‚å¿µçš„ç†è§£ç¨‹åº¦
       - å®é™…é¡¹ç›®ç»éªŒçš„ä½“ç°
    
    2. **é—®é¢˜è§£å†³èƒ½åŠ›** (1-10åˆ†)
       - åˆ†æé—®é¢˜çš„é€»è¾‘æ€§
       - è§£å†³æ–¹æ¡ˆçš„åˆç†æ€§
       - æ€ç»´çš„æ¸…æ™°åº¦
    
    3. **è¡¨è¾¾æ²Ÿé€šèƒ½åŠ›** (1-10åˆ†)
       - å›ç­”çš„æ¡ç†æ€§
       - æŠ€æœ¯è¡¨è¾¾çš„å‡†ç¡®æ€§
       - æ²Ÿé€šçš„æœ‰æ•ˆæ€§
    
    4. **å­¦ä¹ æˆé•¿æ½œåŠ›** (1-10åˆ†)
       - å¯¹æ–°æŠ€æœ¯çš„æ¥å—åº¦
       - æŒç»­å­¦ä¹ çš„æ„æ„¿
       - é€‚åº”èƒ½åŠ›
    
    5. **é¡¹ç›®ç»éªŒåŒ¹é…åº¦** (1-10åˆ†)
       - é¡¹ç›®ç»éªŒä¸å²—ä½çš„åŒ¹é…ç¨‹åº¦
       - å®é™…å·¥ä½œèƒ½åŠ›çš„ä½“ç°
       - å›¢é˜Ÿåä½œç»éªŒ
    
    ã€è¾“å‡ºæ ¼å¼ã€‘
    è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼š
    
    ## ğŸ¯ é¢è¯•è¯„ä¼°æŠ¥å‘Š
    
    ### ğŸ“Š ç»¼åˆè¯„åˆ†
    - **æŠ€æœ¯èƒ½åŠ›**ï¼šX/10åˆ† - ç®€è¦è¯´æ˜
    - **é—®é¢˜è§£å†³èƒ½åŠ›**ï¼šX/10åˆ† - ç®€è¦è¯´æ˜
    - **è¡¨è¾¾æ²Ÿé€šèƒ½åŠ›**ï¼šX/10åˆ† - ç®€è¦è¯´æ˜
    - **å­¦ä¹ æˆé•¿æ½œåŠ›**ï¼šX/10åˆ† - ç®€è¦è¯´æ˜
    - **é¡¹ç›®ç»éªŒåŒ¹é…åº¦**ï¼šX/10åˆ† - ç®€è¦è¯´æ˜
    
    **æ€»ä½“è¯„åˆ†ï¼šXX/50åˆ†**
    
    ### ğŸ’ª ä¸»è¦ä¼˜åŠ¿
    1. ä¼˜åŠ¿ç‚¹1çš„å…·ä½“æè¿°
    2. ä¼˜åŠ¿ç‚¹2çš„å…·ä½“æè¿°
    3. ä¼˜åŠ¿ç‚¹3çš„å…·ä½“æè¿°
    
    ### ğŸ“ˆ æ”¹è¿›å»ºè®®
    1. å…·ä½“çš„æ”¹è¿›å»ºè®®1
    2. å…·ä½“çš„æ”¹è¿›å»ºè®®2
    3. å…·ä½“çš„æ”¹è¿›å»ºè®®3
    
    ### ğŸ¯ å²—ä½åŒ¹é…åº¦
    **åŒ¹é…åº¦ï¼šXX%**
    
    åŒ¹é…åº¦åˆ†æï¼šè¯¦ç»†è¯´æ˜å€™é€‰äººä¸ç›®æ ‡å²—ä½çš„åŒ¹é…æƒ…å†µ
    
    ### ğŸ“ é¢è¯•å®˜æ€»ç»“
    å¯¹å€™é€‰äººçš„æ•´ä½“å°è±¡å’Œæ¨èæ„è§
    
    ---
    
    **ã€é¢è¯•å®˜ã€‘æ„Ÿè°¢æ‚¨å‚åŠ æœ¬æ¬¡æŠ€æœ¯é¢è¯•ï¼**
    
    æœ¬æ¬¡é¢è¯•å·²åœ†æ»¡ç»“æŸã€‚é€šè¿‡{question_count}è½®é—®ç­”ï¼Œæˆ‘ä»¬å¯¹æ‚¨çš„æŠ€æœ¯èƒ½åŠ›å’Œé¡¹ç›®ç»éªŒæœ‰äº†å…¨é¢çš„äº†è§£ã€‚
    
    **é¢è¯•è¡¨ç°æ€»ç»“ï¼š**
    - é¢è¯•æ—¶é•¿ï¼šçº¦{question_count * 3}åˆ†é’Ÿ
    - é—®é¢˜è¦†ç›–ï¼šæŠ€æœ¯åŸºç¡€ã€é¡¹ç›®ç»éªŒã€é—®é¢˜è§£å†³å¤šä¸ªç»´åº¦
    - æ•´ä½“è¡¨ç°ï¼š[æ ¹æ®è¯„åˆ†ç»™å‡ºç®€è¦è¯„ä»·]
    
    æˆ‘ä»¬ä¼šåœ¨3-5ä¸ªå·¥ä½œæ—¥å†…é€šè¿‡é‚®ä»¶æˆ–ç”µè¯çš„æ–¹å¼ç»™æ‚¨åé¦ˆé¢è¯•ç»“æœã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•ç–‘é—®ï¼Œæ¬¢è¿éšæ—¶è”ç³»æˆ‘ä»¬ã€‚
    
    å†æ¬¡æ„Ÿè°¢æ‚¨çš„æ—¶é—´å’Œç²¾å½©è¡¨ç°ï¼Œç¥æ‚¨å·¥ä½œé¡ºåˆ©ï¼
    """
    
    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯èµ„æ·±çš„ITæŠ€æœ¯é¢è¯•å®˜å’ŒHRä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„äººæ‰è¯„ä¼°ç»éªŒã€‚è¯·å®¢è§‚ã€ä¸“ä¸šã€è¯¦ç»†åœ°è¯„ä¼°å€™é€‰äººçš„é¢è¯•è¡¨ç°ã€‚"),
        ("human", evaluation_prompt)
    ])
    
    response = model.invoke(prompt.format_messages())
    
    print(f"[æ—¥å¿—] âœ… æœ€ç»ˆè¯„ä¼°æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    # è¿”å›æœ€ç»ˆçŠ¶æ€
    return {
        "messages": [response],
        "resume_content": resume_content,
        "resume_analyzed": True,
        "question_count": question_count,
        "current_topic": "é¢è¯•è¯„ä¼°",
        "asked_questions": asked_questions,
        "candidate_responses": candidate_responses,
        "interview_feedback": state.get("interview_feedback", []) + [response.content],
        "interview_complete": True
    }
def route_workflow(state):
    """è·¯ç”±å†³å®šä¸‹ä¸€æ­¥æµç¨‹"""
    print(f"[æ—¥å¿—] ğŸ”„ å·¥ä½œæµè·¯ç”±åˆ¤æ–­")
    
    resume_analyzed = state.get("resume_analyzed", False)
    interview_complete = state.get("interview_complete", False)
    question_count = state.get("question_count", 0)
    messages = state.get("messages", [])
    
    print(f"[æ—¥å¿—] ğŸ“Š å½“å‰çŠ¶æ€ - ç®€å†å·²åˆ†æ: {resume_analyzed}, é¢è¯•å®Œæˆ: {interview_complete}, é—®é¢˜æ•°: {question_count}")
    print(f"[è°ƒè¯•] æ¶ˆæ¯æ•°é‡: {len(messages)}")
    
    # çŠ¶æ€æ¢å¤é€»è¾‘
    if len(messages) > 1 and not resume_analyzed:
        print(f"[è­¦å‘Š] æ£€æµ‹åˆ°çŠ¶æ€ä¸¢å¤±ï¼Œå°è¯•æ¢å¤...")
        for msg in messages:
            if hasattr(msg, 'content') and "ã€é¢è¯•å®˜ã€‘" in str(msg.content):
                print(f"[æ¢å¤] å‘ç°å†å²é¢è¯•é—®é¢˜ï¼Œæ¢å¤çŠ¶æ€")
                state["resume_analyzed"] = True
                state["question_count"] = len([m for m in messages if "ã€é¢è¯•å®˜ã€‘" in str(getattr(m, 'content', ''))])
                resume_analyzed = True
                question_count = state["question_count"]
                break
    
    # âœ… å…³é”®ä¿®æ”¹ï¼šåªæœ‰æ˜ç¡®æ ‡è®°å®Œæˆæˆ–ç”¨æˆ·å›ç­”äº†ç¬¬8ä¸ªé—®é¢˜æ‰è¯„ä¼°
    if interview_complete:
        print(f"[æ—¥å¿—] âœ… é¢è¯•æ˜ç¡®æ ‡è®°å®Œæˆï¼Œå¼€å§‹æœ€ç»ˆè¯„ä¼°")
        return "final_evaluation"
    
    # å¦‚æœç®€å†æœªåˆ†æï¼Œå…ˆåˆ†æç®€å†
    if not resume_analyzed:
        print(f"[æ—¥å¿—] ğŸ“„ éœ€è¦åˆ†æç®€å†")
        return "analyze_resume"
    
    # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯çš„ç±»å‹
    if messages:
        last_message = messages[-1]
        last_content = getattr(last_message, 'content', str(last_message))
        
        # å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯æ˜¯é¢è¯•å®˜é—®é¢˜ï¼Œç­‰å¾…ç”¨æˆ·å›ç­”
        if "ã€é¢è¯•å®˜ã€‘" in last_content:
            print(f"[æ—¥å¿—] â¸ï¸ ç­‰å¾…ç”¨æˆ·å›ç­”é—®é¢˜")
            return "END"  # ç­‰å¾…ç”¨æˆ·è¾“å…¥
        
        # âœ… æ–°å¢ï¼šå¦‚æœå·²ç»é—®äº†8ä¸ªé—®é¢˜ä¸”ç”¨æˆ·åˆšå›ç­”ï¼Œè¿›å…¥è¯„ä¼°
        elif question_count >= 8 and not "ã€é¢è¯•å®˜ã€‘" in last_content:
            print(f"[æ—¥å¿—] âœ… ç¬¬8ä¸ªé—®é¢˜å·²å›ç­”ï¼Œå¼€å§‹æœ€ç»ˆè¯„ä¼°")
            return "final_evaluation"
        
        # å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯æ˜¯ç”¨æˆ·å›ç­”ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜
        else:
            print(f"[æ—¥å¿—] ğŸ¤” ç”¨æˆ·å·²å›ç­”ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜")
            return "generate_question"
    
    # é»˜è®¤æƒ…å†µï¼šç”Ÿæˆé—®é¢˜
    print(f"[æ—¥å¿—] ğŸ”„ é»˜è®¤ç”Ÿæˆé—®é¢˜")
    return "generate_question"

# ä¿®æ”¹å·¥ä½œæµå®šä¹‰
workflow = StateGraph(state_schema=InterviewState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("analyze_resume", analyze_resume)
workflow.add_node("generate_question", generate_next_question)
workflow.add_node("final_evaluation", final_evaluation) 
# âœ… å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨æ¡ä»¶è¾¹ä»STARTå¼€å§‹
workflow.add_conditional_edges(
    START,
    route_workflow,  # è®©è·¯ç”±å‡½æ•°å†³å®šèµ·å§‹èŠ‚ç‚¹
    {
        "analyze_resume": "analyze_resume",
        "generate_question": "generate_question",
        "final_evaluation": "final_evaluation",  # æ–°å¢è·¯ç”±
        "END": END
    }
)

# æ·»åŠ æ¡ä»¶è¾¹ï¼šä»åˆ†æç®€å†èŠ‚ç‚¹å‡ºå‘
workflow.add_conditional_edges(
    "analyze_resume",
    route_workflow,
    {
        "generate_question": "generate_question",
        "final_evaluation": "final_evaluation",  # æ–°å¢è·¯ç”±
        "END": END
    }
)

# æ·»åŠ æ¡ä»¶è¾¹ï¼šä»ç”Ÿæˆé—®é¢˜èŠ‚ç‚¹å‡ºå‘
workflow.add_conditional_edges(
    "generate_question",
    route_workflow,
    {
        "generate_question": "generate_question",
        "final_evaluation": "final_evaluation",  # æ–°å¢è·¯ç”±
        "END": END
    }
)
workflow.add_edge("final_evaluation", END)

# ç¼–è¯‘å·¥ä½œæµ
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# æ·»åŠ configé…ç½®
config = {
    "configurable": {
        "session_id": time.time(),
        "thread_id": time.time()
    }
}
