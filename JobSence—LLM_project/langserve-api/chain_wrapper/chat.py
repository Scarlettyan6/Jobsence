from dotenv import load_dotenv, find_dotenv
from langchain_community.chat_models.tongyi import ChatTongyi
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain.tools import Tool
import time
import os
import re
import json

# å°è¯•å¯¼å…¥SerpAPIï¼Œå¦‚æœå¤±è´¥åˆ™æä¾›å¤‡é€‰æ–¹æ¡ˆ
try:
    from langchain_community.utilities import SerpAPIWrapper
    SERPAPI_AVAILABLE = True
except ImportError:
    print("âš ï¸ SerpAPIä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å†…ç½®çŸ¥è¯†å›ç­”é—®é¢˜")
    SERPAPI_AVAILABLE = False

_ = load_dotenv(find_dotenv())

# é…ç½®APIå¯†é’¥
SERPAPI_API_KEY = os.getenv("SERPAPI_API_KEY")

model = ChatTongyi(
    dashscope_api_key="sk-375ffc601cd642598f0cfead988f50ca",
    streaming=True,
    model_name="qwen-turbo"
)

# è”ç½‘æœç´¢åŠŸèƒ½ - å¢å¼ºç‰ˆï¼Œè¿”å›è¯¦ç»†çš„æœç´¢æ¥æºä¿¡æ¯
def web_search(query: str) -> dict:
    """ä½¿ç”¨SerpAPIè¿›è¡Œç½‘ç»œæœç´¢ï¼Œè¿”å›è¯¦ç»†çš„æœç´¢ç»“æœå’Œæ¥æºä¿¡æ¯"""
    try:
        print(f"[æ—¥å¿—] ğŸ” ç½‘ç»œæœç´¢è¢«è°ƒç”¨ï¼Œæœç´¢å†…å®¹ï¼š{query}")
        
        if not SERPAPI_AVAILABLE:
            return {
                "success": False,
                "content": "ç½‘ç»œæœç´¢åŠŸèƒ½æš‚ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥SerpAPIé…ç½®",
                "sources": [],
                "total_results": 0
            }
            
        if not SERPAPI_API_KEY:
            return {
                "success": False,
                "content": "SerpAPIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œç½‘ç»œæœç´¢",
                "sources": [],
                "total_results": 0
            }
            
        search = SerpAPIWrapper()
        # è·å–åŸå§‹æœç´¢ç»“æœ
        raw_result = search.results(query)
        
        # è§£ææœç´¢ç»“æœï¼Œæå–è¯¦ç»†ä¿¡æ¯
        sources = []
        content_parts = []
        
        if "organic_results" in raw_result:
            for i, result in enumerate(raw_result["organic_results"][:8]):  # é™åˆ¶å‰8ä¸ªç»“æœ
                source = {
                    "id": i + 1,
                    "title": result.get("title", "æœªçŸ¥æ ‡é¢˜"),
                    "url": result.get("link", ""),
                    "snippet": result.get("snippet", "æ— æ‘˜è¦"),
                    "displayed_link": result.get("displayed_link", ""),
                    "favicon": result.get("favicon", ""),
                    "position": result.get("position", i + 1)
                }
                sources.append(source)
                content_parts.append(f"[{i+1}] {result.get('title', '')}: {result.get('snippet', '')}")
        
        # å¦‚æœæœ‰çŸ¥è¯†å›¾è°±ç»“æœï¼Œä¹ŸåŒ…å«è¿›æ¥
        if "knowledge_graph" in raw_result:
            kg = raw_result["knowledge_graph"]
            if "description" in kg:
                content_parts.insert(0, f"çŸ¥è¯†å›¾è°±: {kg['description']}")
        
        content = "\n\n".join(content_parts)
        
        print(f"[æ—¥å¿—] âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(sources)} ä¸ªç»“æœ")
        
        return {
            "success": True,
            "content": content,
            "sources": sources,
            "total_results": len(sources),
            "query": query
        }
        
    except Exception as e:
        error_msg = f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        return {
            "success": False,
            "content": error_msg,
            "sources": [],
            "total_results": 0
        }

# æœ¬åœ°èµ„æºæ–‡ä»¶æŸ¥è¯¢åŠŸèƒ½
def query_local_files(query: str) -> dict:
    """æŸ¥è¯¢æœ¬åœ°æ–‡ä»¶ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœ"""
    try:
        print(f"\n[æ—¥å¿—] ğŸ“š æœ¬åœ°æ–‡ä»¶æŸ¥è¯¢ï¼Œå…³é”®è¯ï¼š{query}")
        
        # å®šä¹‰ä¸“ä¸šçš„ITå’¨è¯¢æ•°æ®æ–‡ä»¶è·¯å¾„
        possible_files = [
            # èŒä½å’Œè–ªèµ„æ•°æ®
            "data/bytedance/bytedance_jobs_clean.json",
            "data/bytedance/company_profiles.txt", 
            "data/bytedance/data_statistics.json",
            "data/bytedance/tech_stack_trends.txt",
            
            # ç®€å†ä¼˜åŒ–èµ„æº
            "data/Tencent/tencent_jobs.txt",
            "resources/resume_keywords.txt",
            "resources/project_descriptions.txt",
            "resources/soft_skills_guide.txt",
            
            # æŠ€æœ¯çŸ¥è¯†åº“
            "knowledge/programming_languages.txt",
            "knowledge/frameworks_libraries.txt",
            "knowledge/cloud_technologies.txt",
            "knowledge/devops_tools.txt",
            "knowledge/ai_ml_technologies.txt",
            
            # é¢è¯•æŒ‡å¯¼
            "interview/interview_questions.txt",
            "interview/behavioral_questions.txt",
            "interview/code_challenges.txt",
            "interview/negotiation_tips.txt",
            
            # èŒä¸šå‘å±•
            "career/career_paths.txt",
            "career/skill_roadmaps.txt",
            "career/certifications.txt",
            "career/learning_resources.txt",
        ]
        
        found_content = []
        file_sources = []
        
        for file_path in possible_files:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                    # ç®€å•çš„ç›¸å…³æ€§åŒ¹é…
                    paragraphs = content.split('\n\n')  # æŒ‰æ®µè½åˆ†å‰²
                    relevant_info = []
                    
                    for para in paragraphs:
                        # æ£€æŸ¥æ®µè½æ˜¯å¦åŒ…å«æŸ¥è¯¢å…³é”®è¯
                        if any(keyword in para.lower() for keyword in query.lower().split()):
                            relevant_info.append(para.strip())
                    
                    if relevant_info:
                        found_content.extend(relevant_info)
                        file_sources.append({
                            "file_path": file_path,
                            "file_name": os.path.basename(file_path),
                            "matches": len(relevant_info)
                        })
                        print(f"[æ—¥å¿—] âœ… åœ¨ {file_path} ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
                        
                except Exception as e:
                    print(f"[æ—¥å¿—] âš ï¸ è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
                    continue
        
        if found_content:
            result_content = "\n\n".join(found_content)
            return {
                "success": True,
                "content": f"ä»æœ¬åœ°æ–‡ä»¶ä¸­æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š\n\n{result_content}",
                "sources": file_sources,
                "total_results": len(file_sources)
            }
        else:
            return {
                "success": False,
                "content": "åœ¨æœ¬åœ°æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚å¯èƒ½çš„åŸå› ï¼š\n1. æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨\n2. æ–‡ä»¶ä¸­æ²¡æœ‰åŒ…å«ç›¸å…³å…³é”®è¯çš„å†…å®¹",
                "sources": [],
                "total_results": 0
            }
            
    except Exception as e:
        error_msg = f"æœ¬åœ°æ–‡ä»¶æŸ¥è¯¢å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        return {
            "success": False,
            "content": error_msg,
            "sources": [],
            "total_results": 0
        }

# å®šä¹‰å·¥å…·åˆ—è¡¨
tools = [
    Tool(
        name="web_search",
        func=web_search,
        description="ç”¨äºæœç´¢æœ€æ–°ä¿¡æ¯ã€æ–°é—»å’Œå®æ—¶æ•°æ®ã€‚å½“éœ€è¦è·å–æœ€æ–°ä¿¡æ¯æˆ–ç½‘ç»œä¸Šçš„èµ„æ–™æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚è¾“å…¥ï¼šæœç´¢å…³é”®è¯"
    ),
    Tool(
        name="query_local_files", 
        func=query_local_files,
        description="ç”¨äºæŸ¥è¯¢æœ¬åœ°èµ„æºæ–‡ä»¶ä¸­çš„ä¿¡æ¯ã€‚å½“é—®é¢˜æ¶‰åŠæœ¬åœ°å­˜å‚¨çš„çŸ¥è¯†ã€æ•°æ®æˆ–æ–‡æ¡£æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚è¾“å…¥ï¼šæŸ¥è¯¢å…³é”®è¯"
    )
]

# ä¿®æ”¹å·¥å…·è°ƒç”¨å‡½æ•°ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœ
def call_tools(query: str, tool_name: str) -> dict:
    """æ ¹æ®å·¥å…·åç§°è°ƒç”¨ç›¸åº”çš„å·¥å…·ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœ"""
    for tool in tools:
        if tool.name == tool_name:
            return tool.func(query)  # ç›´æ¥è¿”å›å­—å…¸ç»“æœ
    return {
        "success": False,
        "content": f"æœªæ‰¾åˆ°å·¥å…·: {tool_name}",
        "sources": [],
        "total_results": 0
    }

# ä¿®æ”¹æ™ºèƒ½å·¥å…·é€‰æ‹©å‡½æ•°ï¼Œç®€åŒ–è¾“å‡º
def select_and_call_tool(state: MessagesState) -> dict:
    last_message = state["messages"][-1]
    user_input = last_message.content if hasattr(last_message, 'content') else str(last_message)
    
    # æ‰©å±•å…³é”®è¯åŒ¹é…è§„åˆ™
    web_keywords = ["æœ€æ–°", "æ–°é—»", "å®æ—¶", "å½“å‰", "ä»Šå¤©", "ç°åœ¨", "æœç´¢", "æŸ¥æ‰¾", "ç½‘ä¸Š", "æ‹›è˜", "å²—ä½", "å¸‚åœºè¡Œæƒ…"]
    
    local_keywords = {
        "ç®€å†": ["ç®€å†", "CV", "resume", "æ¨¡æ¿", "ä¼˜åŒ–", "ä¿®æ”¹"],
        "é¢è¯•": ["é¢è¯•", "interview", "é¢˜ç›®", "é—®é¢˜", "å‡†å¤‡"],
        "æŠ€æœ¯": ["æŠ€æœ¯æ ˆ", "ç¼–ç¨‹", "å¼€å‘", "æ¡†æ¶", "è¯­è¨€", "å·¥å…·"],
        "èŒä¸š": ["èŒä¸š", "å‘å±•", "è§„åˆ’", "è·¯å¾„", "æ™‹å‡", "è½¬è¡Œ"],
        "è–ªèµ„": ["è–ªèµ„", "å·¥èµ„", "å¾…é‡", "è–ªé…¬", "è°ˆåˆ¤"],
        "èŒä½": ["èŒä½", "å²—ä½", "æ‹›è˜", "å·¥ä½œ", "å·¥ä½œæœºä¼š","å­—èŠ‚è·³åŠ¨èŒä½","è…¾è®¯èŒä½"],
    }
    
    user_input_lower = user_input.lower()
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
    needs_web_search = any(keyword in user_input_lower for keyword in web_keywords)
    needs_local_search = any(any(kw in user_input_lower for kw in keywords) for keywords in local_keywords.values())
    
    tool_results = []
    all_sources = []  # å­˜å‚¨æ‰€æœ‰æœç´¢æ¥æº
    web_count = 0  # ç½‘é¡µæœç´¢æ•°é‡
    local_count = 0  # æœ¬åœ°æ–‡ä»¶æ•°é‡
    
    if needs_web_search:
        web_result = call_tools(user_input, "web_search")
        if web_result["success"]:
            web_count = web_result.get('total_results', 0)
            # ç®€åŒ–å·¥å…·ç»“æœï¼Œåªä¿ç•™æ ¸å¿ƒå†…å®¹
            tool_results.append(f"ç½‘ç»œæœç´¢ç»“æœï¼š\n{web_result['content']}")
            # ä¸ºç½‘ç»œæœç´¢æ¥æºæ·»åŠ ç±»å‹æ ‡è¯†
            for source in web_result.get('sources', []):
                source['type'] = 'web'
                source['search_query'] = web_result.get('query', user_input)
            all_sources.extend(web_result.get('sources', []))
        else:
            tool_results.append(f"ç½‘ç»œæœç´¢å¤±è´¥ï¼š{web_result.get('content', 'æœªçŸ¥é”™è¯¯')}")
    
    if needs_local_search:
        local_result = call_tools(user_input, "query_local_files")
        if local_result["success"]:
            local_count = local_result.get('total_results', 0)
            tool_results.append(f"æœ¬åœ°æ–‡ä»¶æŸ¥è¯¢ç»“æœï¼š\n{local_result['content']}")
            # ä¸ºæœ¬åœ°æ–‡ä»¶æ¥æºæ·»åŠ ç±»å‹æ ‡è¯†
            for source in local_result.get('sources', []):
                source['type'] = 'local'
            all_sources.extend(local_result.get('sources', []))
    
    # å¦‚æœä½¿ç”¨äº†å·¥å…·ï¼Œå°†ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
    if tool_results:
        # æ„å»ºæœç´¢ç»Ÿè®¡ä¿¡æ¯
        search_stats = []
        
        search_summary = "ã€".join(search_stats) if search_stats else "å·²å®Œæˆæœç´¢"
        
        # ç®€åŒ–æ¶ˆæ¯å†…å®¹ï¼Œä¸åŒ…å«ç”¨æˆ·é—®é¢˜å’Œè¯¦ç»†æœç´¢è¿‡ç¨‹
        tool_info = "\n\n".join(tool_results)
        # ä¿®æ”¹ç¬¬292è¡Œ
        enhanced_message = f""
        # å°†æœç´¢æ¥æºä¿¡æ¯æ·»åŠ åˆ°æ¶ˆæ¯çš„additional_kwargsä¸­
        enhanced_human_message = HumanMessage(
            content=enhanced_message,
            additional_kwargs={
                "search_sources": all_sources,
                "has_sources": len(all_sources) > 0,
                "source_count": len(all_sources),
                "web_count": web_count,
                "local_count": local_count,
                "search_summary": search_summary
            }
        )
        
        return {"messages": [enhanced_human_message]}
    
    return {"messages": [last_message]}

# Define a new graph
workflow = StateGraph(state_schema=MessagesState)

# æ›´æ–°çš„ç³»ç»Ÿæç¤ºè¯
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ITèŒä½å’¨è¯¢å’Œç®€å†å’¨è¯¢åŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

ğŸ¯ **æ ¸å¿ƒä¸“é•¿**ï¼š
- ITèŒä½åˆ†æä¸åŒ¹é…
- ç®€å†ä¼˜åŒ–ä¸æŠ€èƒ½æå‡å»ºè®®  
- æŠ€æœ¯æ ˆè§„åˆ’ä¸å­¦ä¹ è·¯å¾„
- é¢è¯•æŒ‡å¯¼ä¸èŒä¸šå‘å±•
- è–ªèµ„è°ˆåˆ¤ä¸èŒåœºå»ºè®®

ğŸ”§ **å¯ç”¨å·¥å…·**ï¼š
- ç½‘ç»œæœç´¢ï¼šè·å–æœ€æ–°çš„è¡Œä¸šä¿¡æ¯ã€æŠ€æœ¯è¶‹åŠ¿ã€æ‹›è˜éœ€æ±‚
- æœ¬åœ°èµ„æºï¼šæŸ¥è¯¢ä¸“ä¸šçŸ¥è¯†åº“ã€æˆåŠŸæ¡ˆä¾‹ã€æ¨¡æ¿èµ„æº

ğŸ’¡ **å›ç­”åŸåˆ™**ï¼š
1. ä½¿ç”¨ä¸“ä¸šçš„ITæœ¯è¯­å’Œè¡Œä¸šæ ‡å‡†
2. æä¾›å…·ä½“å¯è¡Œçš„å»ºè®®å’Œæ­¥éª¤
3. ç»“åˆæœ€æ–°çš„è¡Œä¸šè¶‹åŠ¿å’ŒæŠ€æœ¯å‘å±•
4. ç»™å‡ºé‡åŒ–çš„æ”¹è¿›å»ºè®®
5. æä¾›ç›¸å…³çš„å­¦ä¹ èµ„æºå’Œå·¥å…·æ¨è
6. å¦‚æœç”¨æˆ·ç»™å‡ºè‡ªå·±çš„æ¯•ä¸šé™¢æ ¡æˆ–åœ°åŒºï¼Œè¯·ç€é‡æŸ¥æ‰¾è¿™ä¸ªåœ°åŒºåŠå…¶é™„è¿‘çš„èŒä½æ¨è
ğŸ“Š **æœç´¢ç»“æœå¤„ç†**ï¼š
- å½“ä½¿ç”¨ç½‘ç»œæœç´¢æ—¶ï¼Œåœ¨å›ç­”å¼€å¤´ç®€å•æ˜¾ç¤ºï¼š"å·²ä¸ºæ‚¨æœç´¢ X ä¸ªç½‘é¡µ"
- ä¸è¦åœ¨å›ç­”ä¸­æ˜¾ç¤ºå…·ä½“çš„æœç´¢é“¾æ¥æˆ–æ¥æºè¯¦æƒ…
- ä¸“æ³¨äºåŸºäºæœç´¢ç»“æœæä¾›æœ‰ä»·å€¼çš„å›ç­”å†…å®¹

è¯·æ ¹æ®ç”¨æˆ·çš„å…·ä½“éœ€æ±‚ï¼Œæä¾›ä¸“ä¸šã€ç®€æ´ã€å®ç”¨çš„å’¨è¯¢å»ºè®®ã€‚
        """),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

# ä¿®æ”¹call_modelå‡½æ•°ï¼Œä¼ é€’æœç´¢æ¥æºä¿¡æ¯
def call_model(state: MessagesState):
    prompt = prompt_template.invoke(state)
    response = model.invoke(prompt)
    
    # æ£€æŸ¥è¾“å…¥æ¶ˆæ¯ä¸­æ˜¯å¦æœ‰æœç´¢æ¥æºä¿¡æ¯
    search_sources = []
    has_sources = False
    web_count = 0
    local_count = 0
    search_summary = ""
    
    for message in state["messages"]:
        if hasattr(message, 'additional_kwargs') and message.additional_kwargs:
            if 'search_sources' in message.additional_kwargs:
                search_sources.extend(message.additional_kwargs['search_sources'])
                has_sources = message.additional_kwargs.get('has_sources', False)
                web_count = message.additional_kwargs.get('web_count', 0)
                local_count = message.additional_kwargs.get('local_count', 0)
                search_summary = message.additional_kwargs.get('search_summary', '')
    
    # å°†æœç´¢æ¥æºä¿¡æ¯æ·»åŠ åˆ°å“åº”æ¶ˆæ¯çš„additional_kwargsä¸­
    if search_sources:
        if hasattr(response, 'additional_kwargs'):
            response.additional_kwargs.update({
                "search_sources": search_sources,
                "has_sources": has_sources,
                "source_count": len(search_sources),
                "web_count": web_count,
                "local_count": local_count,
                "search_summary": search_summary
            })
        else:
            response.additional_kwargs = {
                "search_sources": search_sources,
                "has_sources": has_sources,
                "source_count": len(search_sources),
                "web_count": web_count,
                "local_count": local_count,
                "search_summary": search_summary
            }
    
    return {"messages": response}

# æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
workflow.add_node("tool_selector", select_and_call_tool)
workflow.add_node("model", call_model)

# è®¾ç½®å·¥ä½œæµç¨‹
workflow.add_edge(START, "tool_selector")
workflow.add_edge("tool_selector", "model")

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

config = {
    "configurable": {
        "session_id": time.time(),
        "thread_id": time.time()
    }
}

# æµ‹è¯•å‡½æ•°
# æµ‹è¯•å‡½æ•°
def test_chat_with_tools(question: str):
    """æµ‹è¯•èŠå¤©åŠŸèƒ½"""
    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜ï¼š{question}")
    print("="*50)
    
    try:
        response = app.invoke(
            {"messages": [HumanMessage(content=question)]},
            config=config
        )
        
        last_message = response["messages"][-1]
        answer = last_message.content if hasattr(last_message, 'content') else str(last_message)
        print(f"\nğŸ¤– å›ç­”ï¼š\n{answer}")
        
        # æ˜¾ç¤ºæœç´¢ç»Ÿè®¡å’Œæ¥æºä¿¡æ¯
        if hasattr(last_message, 'additional_kwargs') and last_message.additional_kwargs:
            kwargs = last_message.additional_kwargs
            
            # æ˜¾ç¤ºæœç´¢ç»Ÿè®¡
            if 'search_summary' in kwargs:
                print(f"\nğŸ“Š æœç´¢ç»Ÿè®¡ï¼š{kwargs['search_summary']}")
            
            # æ˜¾ç¤ºè¯¦ç»†æ¥æº
            if 'search_sources' in kwargs:
                sources = kwargs['search_sources']
                if sources:
                    print(f"\nğŸ“š è¯¦ç»†æ¥æº ({len(sources)}ä¸ª):")
                    web_sources = [s for s in sources if s.get('type') == 'web']
                    local_sources = [s for s in sources if s.get('type') == 'local']
                    
                    if web_sources:
                        print(f"\nğŸŒ ç½‘é¡µæ¥æº ({len(web_sources)}ä¸ª):")
                        for i, source in enumerate(web_sources, 1):
                            print(f"  {i}. [{source.get('title', 'æœªçŸ¥æ ‡é¢˜')}]({source.get('url', '')})")
                            print(f"     æ‘˜è¦: {source.get('snippet', 'æ— æ‘˜è¦')[:100]}...")
                    
                    if local_sources:
                        print(f"\nğŸ“ æœ¬åœ°æ–‡ä»¶ ({len(local_sources)}ä¸ª):")
                        for i, source in enumerate(local_sources, 1):
                            print(f"  {i}. æ–‡ä»¶: {source.get('file_name', 'æœªçŸ¥æ–‡ä»¶')}")
                            print(f"     åŒ¹é…æ•°: {source.get('matches', 0)}")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
    
    print("="*50)

if __name__ == "__main__":
    print("ğŸš€ ITèŒä½å’¨è¯¢åŠ©æ‰‹å·²å¯åŠ¨...")
    print("\nğŸ“Œ å¼€å§‹æµ‹è¯•...")
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "è¯·æœç´¢2024å¹´Pythonå¼€å‘å·¥ç¨‹å¸ˆçš„æœ€æ–°æ‹›è˜è¦æ±‚",
        "æŸ¥è¯¢æœ¬åœ°æ–‡ä»¶ä¸­å…³äºç®€å†ä¼˜åŒ–çš„å»ºè®®",
        "æˆ‘æƒ³äº†è§£å‰ç«¯å¼€å‘çš„æŠ€æœ¯æ ˆå‘å±•è¶‹åŠ¿"
    ]
    
    for question in test_questions:
        test_chat_with_tools(question)
